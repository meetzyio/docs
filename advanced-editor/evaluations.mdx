---
title: 'Evaluations'
description: 'Create automated quality tests for your AI agent'
icon: 'check-double'
---

## What are Evaluations?

Evaluations are automated tests that assess your agent's performance after each call. They help you measure success rates, identify issues, and maintain quality standards across all conversations.

<Info>
Evaluations provide quantitative metrics to track your agent's performance over time.
</Info>

## How Evaluations Work

Each evaluation is a **yes/no question** that the AI analyzes based on the conversation transcript. After every call, Meetzy runs all configured evaluations and stores the results.

### Example Evaluations

| Evaluation | Question |
|------------|----------|
| Appointment Scheduled | Did the customer schedule an appointment? |
| Customer Satisfied | Did the customer express satisfaction with the service? |
| Issue Resolved | Was the customer's issue successfully resolved? |
| Information Collected | Did the agent collect all required information? |
| Objection Handled | Did the agent successfully address customer objections? |

## AI Copilot Suggestions

<Info>
The AI Copilot can automatically suggest performance metrics and evaluation criteria based on your agent's purpose and conversation patterns.
</Info>

### Automatic Evaluation Recommendations

The AI Copilot analyzes your agent configuration and suggests relevant evaluations:

1. **Context-Aware Suggestions**: Based on your agent's role (sales, support, appointments)
2. **Conversation Analysis**: Recommendations from analyzing test conversations
3. **Industry Best Practices**: Standard metrics for your use case
4. **Performance Optimization**: Evaluations that identify improvement opportunities

### Using Copilot Suggestions

<Steps>
  <Step title="Open AI Copilot">
    Click the sparkles icon in the right panel to access AI Copilot.
  </Step>
  <Step title="Request Evaluation Suggestions">
    Type something like "Suggest evaluations for my agent" or use the quick actions.
  </Step>
  <Step title="Review Recommendations">
    The Copilot will analyze your agent and suggest specific evaluation criteria.
  </Step>
  <Step title="Accept or Customize">
    Accept suggestions directly or modify them to fit your specific needs.
  </Step>
</Steps>

### Copilot Suggestion Types

<Tabs>
  <Tab title="Goal-Based">
    Evaluations focused on primary objectives:
    - "Did the agent achieve the call objective?"
    - "Was the primary goal accomplished?"
    - "Did the conversation meet success criteria?"
  </Tab>
  <Tab title="Quality Metrics">
    Performance and professionalism evaluations:
    - "Did the agent maintain professional tone?"
    - "Was the customer treated courteously?"
    - "Did the agent follow the conversation script?"
  </Tab>
  <Tab title="Compliance Checks">
    Regulatory and policy adherence:
    - "Were required disclosures provided?"
    - "Did the agent follow compliance protocols?"
    - "Was consent properly obtained?"
  </Tab>
  <Tab title="Data Collection">
    Information gathering effectiveness:
    - "Was all required information collected?"
    - "Did the agent verify customer details?"
    - "Were preferences and requirements captured?"
  </Tab>
</Tabs>

### Applying Copilot Suggestions

When the AI Copilot suggests evaluations, they appear in a purple suggestion banner:

<Warning>
Review Copilot suggestions carefully before accepting them. While AI recommendations are based on best practices, they should be customized for your specific use case.
</Warning>

<Steps>
  <Step title="Review Suggestions">
    Examine the suggested evaluation name and question for relevance.
  </Step>
  <Step title="Customize as Needed">
    Modify the evaluation question to match your specific requirements.
  </Step>
  <Step title="Accept or Reject">
    Use the "Accept All" button to add all suggestions, or reject to dismiss them.
  </Step>
  <Step title="Fine-tune">
    Edit accepted evaluations in the main interface to perfect them.
  </Step>
</Steps>

## Creating Evaluations

<Steps>
  <Step title="Open Evaluations Section">
    Navigate to the Evaluations section in the left panel (‚úÖ icon).
  </Step>
  <Step title="Add Evaluation">
    Click "Add Evaluation" to create a new test.
  </Step>
  <Step title="Enter Name">
    Give your evaluation a descriptive name.
  </Step>
  <Step title="Write Question">
    Create a clear yes/no question that can be answered from the transcript.
  </Step>
  <Step title="Save">
    Save the evaluation to add it to your agent.
  </Step>
</Steps>

### Evaluation Properties

| Property | Description | Example |
|----------|-------------|---------|
| Name | Short identifier | "Appointment Scheduled" |
| Question | Yes/no prompt for AI | "Did the customer schedule an appointment during this call?" |

## Writing Effective Questions

<Info>
The clearer your question, the more accurate the evaluation results will be.
</Info>

### Good Questions

- ‚úÖ "Did the customer agree to schedule a follow-up call?"
- ‚úÖ "Did the agent verify the customer's contact information?"
- ‚úÖ "Was the customer's primary concern addressed during the call?"
- ‚úÖ "Did the agent mention the current promotion?"

### Poor Questions

- ‚ùå "Was the call good?" (too vague)
- ‚ùå "How satisfied was the customer?" (not yes/no)
- ‚ùå "Rate the agent's performance" (not answerable from transcript)

### Copilot-Enhanced Question Writing

The AI Copilot can help improve your evaluation questions:

1. **Clarity Enhancement**: Makes vague questions more specific
2. **Binary Conversion**: Converts complex questions into yes/no format
3. **Context Addition**: Adds relevant context for better evaluation accuracy
4. **Best Practice Alignment**: Ensures questions follow evaluation best practices

## Using Templates

Meetzy provides pre-built evaluation templates for common use cases:

<Tabs>
  <Tab title="Sales">
    - "Did the customer express interest in the product?"
    - "Was pricing discussed during the call?"
    - "Did the agent attempt to close the sale?"
    - "Were all customer objections addressed?"
  </Tab>
  <Tab title="Support">
    - "Was the customer's issue resolved?"
    - "Did the agent follow the troubleshooting protocol?"
    - "Was an escalation necessary?"
    - "Did the customer confirm their issue was fixed?"
  </Tab>
  <Tab title="Appointments">
    - "Was an appointment successfully scheduled?"
    - "Did the agent confirm the appointment details?"
    - "Were alternative times offered if the preferred slot was unavailable?"
    - "Did the customer receive confirmation of the appointment?"
  </Tab>
</Tabs>

### Applying Templates

<Steps>
  <Step title="Click Templates">
    Click the "Templates" button in the Evaluations section.
  </Step>
  <Step title="Browse Categories">
    Select a category relevant to your use case.
  </Step>
  <Step title="Select Evaluations">
    Choose which evaluations to add.
  </Step>
  <Step title="Customize">
    Modify the questions to match your specific needs.
  </Step>
</Steps>

## Managing Evaluations

### Editing Evaluations

1. Click the edit icon (‚úèÔ∏è) next to any evaluation
2. Modify the name or question
3. Save changes

### Deleting Evaluations

1. Click the delete icon (üóëÔ∏è) next to the evaluation
2. Confirm deletion

<Warning>
Deleting an evaluation removes it from future calls. Historical evaluation data is preserved.
</Warning>

### Reordering Evaluations

Drag and drop evaluations to change their display order.

## Viewing Results

Evaluation results are available in multiple places:

| Location | What You See |
|----------|--------------|
| Call Details | Individual evaluation results for each call |
| Analytics Dashboard | Aggregated success rates over time |
| Webhook Output | Raw evaluation data sent to your systems |
| API Response | Programmatic access to results |

## Best Practices

### Start with Copilot Suggestions

<Info>
Begin by asking the AI Copilot to suggest evaluations. This provides a solid foundation based on your agent's configuration and industry best practices.
</Info>

### Evaluation Quality Guidelines

1. **Be Specific**: The more specific your question, the more reliable the evaluation
2. **Avoid Overlap**: Don't create multiple evaluations that measure the same thing
3. **Start Simple**: Begin with 3-5 core evaluations that measure your most important KPIs
4. **Review Regularly**: Periodically review evaluation results to calibrate questions
5. **Use for Training**: Low-scoring evaluations highlight areas for agent improvement

### Copilot Integration Tips

- **Iterate with AI**: Use the Copilot to refine and improve existing evaluations
- **Context Matters**: Provide context about your business when asking for suggestions
- **Test Suggestions**: Use the Playground to validate Copilot-suggested evaluations
- **Combine Sources**: Mix Copilot suggestions with templates and custom evaluations

## Common Evaluation Patterns

### Goal Achievement
```
Name: Primary Goal Met
Question: Did the call achieve its primary objective (sale, appointment, resolution, etc.)?
```

### Compliance
```
Name: Disclosure Made
Question: Did the agent provide the required legal disclosure at the beginning of the call?
```

### Quality
```
Name: Professional Tone
Question: Did the agent maintain a professional and courteous tone throughout the call?
```

### Data Collection
```
Name: Contact Info Verified
Question: Did the agent verify or collect the customer's email and phone number?
```

## Next Steps

<CardGroup cols={2}>
  <Card
    title="AI Copilot"
    icon="sparkles"
    href="/advanced-editor/ai-copilot"
  >
    Get AI-powered evaluation suggestions
  </Card>
  <Card
    title="Playground"
    icon="gamepad"
    href="/advanced-editor/playground"
  >
    Test evaluations in real-time
  </Card>
  <Card
    title="Output Settings"
    icon="arrow-right-from-bracket"
    href="/advanced-editor/output-settings"
  >
    Include evaluations in webhooks
  </Card>
  <Card
    title="Analytics"
    icon="chart-bar"
    href="/analytics"
  >
    View evaluation results and trends
  </Card>
</CardGroup>